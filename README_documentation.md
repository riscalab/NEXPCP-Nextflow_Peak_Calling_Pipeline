# peak_calling_workflow
# peak_calling_workflow

## commands used to get Hera's broadPeaks
"ls /lustre/fs4/risc_lab/scratch/hcanaj/K562_cutandtag_H1low_alignedbam_012925_hg38/all_aligned_bams_bw_hg38/Peak_Call_cutandtag_hg38/broad/H1low_H3k27me3_*[r1,r2,r3]*_001.trim.st.all.blft.qft.rmdup.sorted.bam_peaks.broadPeak | xargs cp -t ."

"ls /lustre/fs4/risc_lab/scratch/hcanaj/K562_cutandtag_H1low_alignedbam_012925_hg38/all_aligned_bams_bw_hg38/Peak_Call_cutandtag_hg38/broad/Scrm_H3k27me3_*[r1,r2,r3]*_001.trim.st.all.blft.qft.rmdup.sorted.bam_peaks.broadPeak| xargs cp -t ."

## path where I got Hera's bam files

"this is the path where i got hera's bam files from: /lustre/fs4/risc_lab/scratch/hcanaj/K562_cutandtag_H1low_alignedbam_012925_hg38/all_aligned_bams_bw_hg38"

## this is also where to find the duplicate information for each of the replicates
"/lustre/fs4/risc_lab/scratch/hcanaj/K562_cutandtag_H1low_alignedbam_012925_hg38/all_aligned_bams_bw_hg38"
then go into the directory for each replicate and find the duplicate log file "*dups.log"

# use this command to copy them to the current directory you are in
"ls /lustre/fs4/risc_lab/scratch/hcanaj/K562_cutandtag_{H1low,Scrm}_alignedbam_012925_hg38/*_H3k27me3_{r1,r2,r3}/*_dups.log | xargs cp -t ."

# I need to get the other log files in the directory
ls /lustre/fs4/risc_lab/scratch/hcanaj/K562_cutandtag_{H1low,Scrm}_alignedbam_012925_hg38/*_{H3k27me3,H3k9me3,H3k36me2}_{r1,r2,r3}/*_dups.log | xargs cp -t .



# I want to check encodes data and run it through my pipeline to see the type of peaks called using my settings so here is how i downloaded it (cant test on public data because its single end not pairend)
" this is k562 H3k27me3 cell histone data from encode"
wget https://www.encodeproject.org/files/ENCFF915XIL/@@download/ENCFF915XIL.bam

"using this one from bernstein zkg is bio rep 1czd is bio rep 2"
wget https://www.encodeproject.org/files/ENCFF392ZKG/@@download/ENCFF392ZKG.bam
wget https://www.encodeproject.org/files/ENCFF905CZD/@@download/ENCFF905CZD.bam


" next i want to change the name to have the correct order"
mv ENCFF392ZKG.bam published_H3k27me3_r1_s1_ENCFF392ZKG.bam
mv ENCFF905CZD.bam published_H3k27me3_r2_s2_ENCFF905CZD.bam

"then for it to work in my pipeline seemlessly, I need to make the index for the bam"
"first sort the bam "
conda activate samtools-1.21_rj 

samtools sort published_H3k27me3_r1_s1_ENCFF392ZKG.bam -o published_H3k27me3_r1_s1_ENCFF392ZKG.sorted.bam
samtools index published_H3k27me3_r1_s1_ENCFF392ZKG.sorted.bam --bai

samtools sort published_H3k27me3_r2_s2_ENCFF905CZD.bam -o published_H3k27me3_r2_s2_ENCFF905CZD.sorted.bam
"then index"
samtools index published_H3k27me3_r2_s2_ENCFF905CZD.sorted.bam --bai


## Now I need to get the 3 control files from geo ( look in the h1 channel) and run them through my fastq2bam nextflow pipeline and then bring the sorted bam and bam index files back here to run through the peak calling pipeline

**this is how I get the GEO data**
```
/lustre/fs4/home/rjohnson/pipelines/peak_calling_analysis_pipeline/test_published_data/get_sra_published_data.sh
```

**Then put the data in this directory**

```
./sra_data
```

**Next I made a new dir to store the nextflow fastq2bam pipeline**

```
nexdep_get_bams/NEXDEP-Nextflow_DNA_Epigenomic_Pipeline
```

**In the nexdep pipeline, I decide to edit the sbatch script file to take the fastq files I made and to use the hg38 genome and blacklist file**

```
edit this file: run_fastq2bam_nf_pipeline.sh

Code to run with new GEO data:

nextflow run fastq2bam_nextflow_pipeline.nf -profile 'fastq2bam2_pipeline' \
-resume \
--genome '/lustre/fs4/risc_lab/store/risc_data/downloaded/hg38/genome/Sequence/WholeGenomeFasta/genome.fa' \
--PE \
--BL \
--blacklist_path '/lustre/fs4/risc_lab/store/risc_data/downloaded/hg38/blacklist/hg38-blacklist.v2.bed' \
--paired_end_reads '/ru-auth/local/home/rjohnson/pipelines/peak_calling_analysis_pipeline/test_published_data/sra_data/*{_1,_2}*' \
--use_effectiveGenomeSize \
--num_effectiveGenomeSize '2864785220' 

```

**Had a great idea to find all the .command.sh files generated by the pipeline and put them in a txt file for documentation on what commands were used under the hood of my nextflow pipeline**

```
find ./work -type f -name ".command.sh" | xargs cat

-or-

find ./work -type f -name ".command.sh" | xargs cat > command_documentation.txt

```

**However, nextflow has nextflow log options and you can see this and a template to generate comprehensive reports on the nextflow documentation website under "running pipelines" and "reports" sections**

**make a template file containing this called my_template.md**
```
## $name

script:

    $script

exist status: $exit
task status: $status

# not this, it doesnt work, maybe becasue I don't make use of a folder???
#task folder: $folder

```

**You have to specify the run name so this might not be automated. In this case soggy_dijkstra**
```
 nextflow log soggy_dijkstra -t my_template.md > output_report{run name}.md

```

**easiest thing to do is just to run nextflow with the -with-report flag and put the name of the output file behind the flag**
```
nextflow run <pipeline> -with-report [file name]
```



**Here is how I made the masterPeak**
**This shows that i get the same number of lines as the mPeak file in Granges before reducing to get the masterPeak in R**
```
[rjohnson@node214 test_published_data]$ cat ./idr_results/H3k27me3/H1low/concat_IDR_H1low_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak ./idr_results/H3k27me3/control/concat_IDR_control_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak > test_master_peak_h1low_geo_control.broadPeak


wc -l test_master_peak_h1low_geo_control.broadPeak 
215680 test_master_peak_h1low_geo_control.broadPeak
[rjohnson@node214 test_published_data]$ wc -l ./idr_results/H3k27me3/control/concat_IDR_control_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak
105261 ./idr_results/H3k27me3/control/concat_IDR_control_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak
[rjohnson@node214 test_published_data]$ wc -l ./idr_results/H3k27me3/H1low/concat_IDR_H1low_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak
110419 ./idr_results/H3k27me3/H1low/concat_IDR_H1low_H3k27me3_r1_vs_r2_vs_r3_0.4_pairs.broadPeak

```

# testing how to get area under the curve of the profile plots using ucsc tools

#  Each *.tab file has columns:
#    name  size  covered  mean0  mean  meanNonZero  sum
#  “sum” is literally the sum of signal across the region (AUC).

```
conda activate uscs_utils_rj 


# first needed to remove the columns after 3rd column because the tool expects the 4th to be names of the regions and different
# so this is just a test I did 
awk 'BEGIN{i=1} {print $1, $2, $3, "peak" i; i++}' OFS="\t" /lustre/fs4/home/rjohnson/pipelines/peak_calling_analysis_pipeline/test_published_data/up_regulated_peaks.bed \
  > up_peaks_named.bed



bigWigAverageOverBed  /lustre/fs4/home/rjohnson/pipelines/peak_calling_analysis_pipeline/bigwigs/H3k27me3_bigwigs/H1low_H3k27me3_r1_S25_001.trim.st.all.blft.qft.rmdup.sorted_cpm_normalized.bigwig  up_peaks_named.bed H1low_H3k27me3_r1_auc.tab

# now with 50kb around the center for up peaks

bigWigAverageOverBed -sampleAroundCenter=50000  /lustre/fs4/home/rjohnson/pipelines/peak_calling_analysis_pipeline/bigwigs/H3k27me3_bigwigs/H1low_H3k27me3_r1_S25_001.trim.st.all.blft.qft.rmdup.sorted_cpm_normalized.bigwig up_peaks_named.bed  H1low_H3k27me3_r1_50kb_auc.tab


# now do this for scrm rep 1 in up peaks to then compare if peak 1 lost signal or gained and by how much

bigWigAverageOverBed -sampleAroundCenter=50000  /lustre/fs4/home/rjohnson/pipelines/peak_calling_analysis_pipeline/bigwigs/H3k27me3_bigwigs/Scrm_H3k27me3_r1_S1_001.trim.st.all.blft.qft.rmdup.sorted_cpm_normalized.bigwig up_peaks_named.bed  scrm_H3k27me3_r1_50kb_auc.tab


```